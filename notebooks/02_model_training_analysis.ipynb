{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767130fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SurvivorDataProcessor imported successfully\n",
      "‚úÖ All modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project to path\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "print(\"üèùÔ∏è Survivor Model Training & Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    from model_trainer import SurvivorModelTrainer\n",
    "    from model_evaluator import SurvivorModelEvaluator\n",
    "    print(\"‚úÖ All modules imported successfully\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = SurvivorModelTrainer()\n",
    "    \n",
    "    # Train all models (this may take a few minutes)\n",
    "    print(\"üöÄ Starting model training...\")\n",
    "    results = trainer.train_all_models()\n",
    "    \n",
    "    if results:\n",
    "        print(\"‚úÖ Training completed successfully!\")\n",
    "        print(f\"‚úÖ Trained {sum(len(task_results) for task_results in results.values())} models\")\n",
    "    else:\n",
    "        print(\"‚ùå Training failed!\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Make sure you're running from the notebooks/ directory\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70873e22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get performance comparison\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mresults\u001b[49m:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìä MODEL PERFORMANCE COMPARISON\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m40\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# Get performance comparison\n",
    "if results:\n",
    "    print(\"üìä MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    comparison_df = trainer.get_model_comparison()\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Plot comparison\n",
    "    trainer.plot_model_comparison()\n",
    "    \n",
    "    # Print insights\n",
    "    print(\"\\nüí° Key Insights:\")\n",
    "    \n",
    "    # Best models for each task\n",
    "    for task in comparison_df['task'].unique():\n",
    "        task_data = comparison_df[comparison_df['task'] == task]\n",
    "        best_model = task_data.loc[task_data['score'].idxmax()]\n",
    "        print(f\"   ‚Ä¢ Best {task.replace('_', ' ')}: {best_model['model']} ({best_model['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance for key predictions\n",
    "key_tasks = ['merge_prediction', 'finale_prediction', 'winner_prediction']\n",
    "\n",
    "for task in key_tasks:\n",
    "    print(f\"\\nüîç {task.replace('_', ' ').upper()} - TOP FEATURES\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Show Random Forest features (usually most interpretable)\n",
    "    features = trainer.get_top_features(task, 'random_forest', top_n=10)\n",
    "    if features is not None:\n",
    "        display(features)\n",
    "        trainer.plot_feature_importance(task, 'random_forest', top_n=15)\n",
    "    else:\n",
    "        print(\"No feature importance available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f12243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = SurvivorModelEvaluator()\n",
    "\n",
    "# Performance summary\n",
    "summary = evaluator.create_performance_summary()\n",
    "print(\"üéØ COMPREHENSIVE PERFORMANCE SUMMARY\")\n",
    "display(summary)\n",
    "\n",
    "# Create detailed visualizations\n",
    "print(\"\\nüìä Creating detailed evaluation plots...\")\n",
    "\n",
    "# Confusion matrices\n",
    "evaluator.plot_confusion_matrices()\n",
    "\n",
    "# ROC curves\n",
    "evaluator.plot_roc_curves()\n",
    "\n",
    "# Prediction vs actual for regression\n",
    "evaluator.plot_prediction_vs_actual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6300e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze overall feature importance patterns\n",
    "print(\"üß† FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "importance_df = evaluator.analyze_feature_importance_patterns()\n",
    "\n",
    "if importance_df is not None:\n",
    "    print(\"Top 15 Most Important Features Across All Models:\")\n",
    "    display(importance_df[['mean_importance']].head(15))\n",
    "    \n",
    "    # Analyze feature categories\n",
    "    print(\"\\nüìã Feature Category Analysis:\")\n",
    "    \n",
    "    def categorize_feature(feature_name):\n",
    "        if any(x in feature_name.lower() for x in ['challenge', 'win_rate', 'tribal', 'individual']):\n",
    "            return 'Challenge Performance'\n",
    "        elif any(x in feature_name.lower() for x in ['age', 'gender', 'fitness', 'athletic', 'physical']):\n",
    "            return 'Demographics/Physical'\n",
    "        elif any(x in feature_name.lower() for x in ['strategic', 'alliance', 'advantage', 'votes']):\n",
    "            return 'Strategic Gameplay'\n",
    "        elif any(x in feature_name.lower() for x in ['occupation', 'home', 'region', 'relationship']):\n",
    "            return 'Background/Social'\n",
    "        elif any(x in feature_name.lower() for x in ['knowledge', 'survivor']):\n",
    "            return 'Game Knowledge'\n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    # Apply categorization\n",
    "    top_20_features = importance_df.head(20).copy()\n",
    "    top_20_features['category'] = top_20_features.index.map(categorize_feature)\n",
    "    \n",
    "    # Group by category\n",
    "    category_importance = top_20_features.groupby('category')['mean_importance'].agg(['sum', 'count', 'mean']).round(3)\n",
    "    category_importance.columns = ['Total_Importance', 'Feature_Count', 'Avg_Importance']\n",
    "    category_importance = category_importance.sort_values('Total_Importance', ascending=False)\n",
    "    \n",
    "    display(category_importance)\n",
    "    \n",
    "    # Plot category importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(category_importance.index, category_importance['Total_Importance'], \n",
    "            alpha=0.7, color='steelblue')\n",
    "    plt.title('Feature Importance by Category (Top 20 Features)')\n",
    "    plt.xlabel('Feature Category')\n",
    "    plt.ylabel('Total Importance Score')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survivor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
